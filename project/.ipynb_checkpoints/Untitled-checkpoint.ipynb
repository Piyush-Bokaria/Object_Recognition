{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62f5cc7-946b-42af-a0d4-44a14463ab1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyttsx3\n",
      "  Downloading pyttsx3-2.98-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting comtypes (from pyttsx3)\n",
      "  Downloading comtypes-1.4.8-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pypiwin32 (from pyttsx3)\n",
      "  Downloading pypiwin32-223-py3-none-any.whl.metadata (236 bytes)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\piyush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyttsx3) (306)\n",
      "Downloading pyttsx3-2.98-py3-none-any.whl (34 kB)\n",
      "Downloading comtypes-1.4.8-py3-none-any.whl (229 kB)\n",
      "Downloading pypiwin32-223-py3-none-any.whl (1.7 kB)\n",
      "Installing collected packages: pypiwin32, comtypes, pyttsx3\n",
      "Successfully installed comtypes-1.4.8 pypiwin32-223 pyttsx3-2.98\n"
     ]
    }
   ],
   "source": [
    "!pip install pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41391194-f49a-4fd0-b28c-68c3856003f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Piyush\\Social_Relevant_Project\\project\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Piyush\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d118f1a1-9f52-44cf-97f2-81c9dba9084e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.25  Python-3.12.1 torch-2.4.1+cpu CPU (AMD Ryzen 5 5600H with Radeon Graphics)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n-2.pt, data=indoor_datasets/data.yaml, epochs=50, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train30, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=C:\\Users\\Piyush\\runs\\detect\\train30\n",
      "Overriding model.yaml nc=80 with nc=25\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    756187  ultralytics.nn.modules.head.Detect           [25, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3,015,723 parameters, 3,015,707 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Piyush\\Social_Relevant_Project\\project\\indoor_datasets\\train\\labels.cache... 4562 images, 41 b\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\Piyush\\Social_Relevant_Project\\project\\indoor_datasets\\train\\images\\imagen_2024-07-14_19-29-32_png.rf.3af11b0783d545d791160e9dcb20e032.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\Piyush\\Social_Relevant_Project\\project\\indoor_datasets\\train\\images\\imagen_2024-07-14_19-29-32_png.rf.bc2e79f913549e0511fbc36732b5e497.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\Piyush\\Social_Relevant_Project\\project\\indoor_datasets\\train\\images\\imagen_2024-07-14_21-43-47_png.rf.ab8ecc191dbd52e7b55d61f6e0de223a.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\Piyush\\Social_Relevant_Project\\project\\indoor_datasets\\train\\images\\imagen_2024-07-17_02-19-05_png.rf.b8116c8c44bb5c705676fad72242dfdc.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\Piyush\\Social_Relevant_Project\\project\\indoor_datasets\\train\\images\\imagen_2024-07-17_02-19-05_png.rf.c370da14960561cebd412ab8c10ffa14.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\Piyush\\Social_Relevant_Project\\project\\indoor_datasets\\train\\images\\imagen_2024-07-17_02-19-05_png.rf.e3828075a93d1cace447c7a2d4221f09.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\Piyush\\Social_Relevant_Project\\project\\indoor_datasets\\train\\images\\imagen_2024-07-27_13-45-09_png.rf.29dffc2354249ac57e8e2ab55d592627.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\Piyush\\Social_Relevant_Project\\project\\indoor_datasets\\train\\images\\imagen_2024-07-27_13-45-09_png.rf.2c9a52d49515213fa0dd6724aa32066e.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Piyush\\Social_Relevant_Project\\project\\indoor_datasets\\valid\\labels.cache... 438 images, 8 backg\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to C:\\Users\\Piyush\\runs\\detect\\train30\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000345, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\Piyush\\runs\\detect\\train30\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50         0G     0.8284      3.418      1.151         13        640: 100%|██████████| 286/286 [31:05<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [01:11"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.676      0.474      0.506      0.401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50         0G     0.7695      1.996      1.111         17        640: 100%|██████████| 286/286 [30:04<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [01:10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.664      0.645      0.667       0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50         0G     0.7291      1.601      1.088          6        640: 100%|██████████| 286/286 [29:58<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.724      0.672      0.719       0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50         0G     0.7104      1.439      1.075         28        640: 100%|██████████| 286/286 [22:05<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.771      0.728      0.771      0.617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50         0G     0.6825      1.294      1.055         10        640: 100%|██████████| 286/286 [22:12<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922       0.76       0.77      0.793      0.633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50         0G     0.6679      1.208      1.044         10        640: 100%|██████████| 286/286 [22:05<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.784      0.783      0.813      0.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50         0G     0.6526      1.129      1.034          7        640: 100%|██████████| 286/286 [22:11<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.817      0.769      0.814      0.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50         0G     0.6446      1.088      1.034         18        640: 100%|██████████| 286/286 [22:04<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.816      0.804      0.834      0.678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50         0G     0.6303      1.038      1.025         10        640: 100%|██████████| 286/286 [22:03<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:49"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.834      0.782      0.831      0.677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50         0G     0.6203     0.9968      1.021         12        640: 100%|██████████| 286/286 [22:54<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [01:10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.785      0.803      0.832      0.678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50         0G     0.6099     0.9506      1.017         25        640: 100%|██████████| 286/286 [30:35<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [01:11"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.809      0.806      0.834       0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50         0G      0.606      0.921      1.011          8        640: 100%|██████████| 286/286 [31:22<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [01:12"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.822      0.813      0.838      0.686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50         0G     0.6017     0.9012      1.008         29        640: 100%|██████████| 286/286 [26:29<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:48"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.821      0.809      0.845      0.691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50         0G      0.593     0.8691      1.006          8        640: 100%|██████████| 286/286 [21:42<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:49"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.842      0.821      0.847      0.695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50         0G     0.5911      0.852      1.005         22        640: 100%|██████████| 286/286 [21:45<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:48"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.833      0.817      0.843      0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50         0G     0.5815     0.8272     0.9971          8        640: 100%|██████████| 286/286 [21:42<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:49"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.822       0.81      0.847      0.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50         0G     0.5832     0.8155     0.9973         20        640: 100%|██████████| 286/286 [21:39<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:48"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.846       0.82      0.852      0.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50         0G     0.5751     0.8059      0.993         27        640: 100%|██████████| 286/286 [21:39<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:48"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.829      0.833      0.852      0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50         0G     0.5711     0.7809     0.9924         22        640: 100%|██████████| 286/286 [21:41<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:49"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.834      0.832       0.85      0.704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50         0G     0.5594     0.7654     0.9856         25        640: 100%|██████████| 286/286 [21:40<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:48"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.849       0.82      0.853      0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50         0G      0.558     0.7543     0.9859         24        640: 100%|██████████| 286/286 [21:46<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:48"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.827      0.844       0.86      0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50         0G     0.5584     0.7311     0.9829         11        640: 100%|██████████| 286/286 [21:35<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:48"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.828      0.824      0.853      0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50         0G     0.5574     0.7343     0.9855         11        640: 100%|██████████| 286/286 [21:35<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:48"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.819      0.843      0.857      0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50         0G     0.5499     0.7183     0.9802         32        640: 100%|██████████| 286/286 [21:37<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:48"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922       0.86      0.821      0.866      0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50         0G     0.5446      0.713     0.9776          8        640: 100%|██████████| 286/286 [21:36<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:48"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.848       0.82      0.857      0.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50         0G     0.5411     0.6887     0.9731          7        640: 100%|██████████| 286/286 [21:36<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:48"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.834      0.835      0.862      0.715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50         0G     0.5343     0.6819     0.9717         31        640: 100%|██████████| 286/286 [29:34<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [01:08"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.848      0.824      0.862      0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50         0G      0.528      0.671     0.9686         10        640: 100%|██████████| 286/286 [29:55<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [01:08"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.855      0.835      0.863      0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50         0G     0.5245     0.6617     0.9662         17        640: 100%|██████████| 286/286 [29:57<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [01:08"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922       0.85      0.838      0.866      0.723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50         0G     0.5239     0.6563     0.9666         15        640: 100%|██████████| 286/286 [30:08<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [01:09"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.839      0.834      0.863       0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50         0G     0.5251     0.6511     0.9669         18        640: 100%|██████████| 286/286 [24:04<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:50"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922       0.84      0.837       0.86      0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50         0G     0.5201      0.639     0.9658          6        640: 100%|██████████| 286/286 [22:42<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:51"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.841      0.834      0.866      0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50         0G     0.5149     0.6295     0.9648          9        640: 100%|██████████| 286/286 [22:45<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [00:51"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.835      0.844      0.863       0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50         0G     0.5111     0.6216     0.9627         13        640: 100%|██████████| 286/286 [22:46<00:00,  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 14/14 [01:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        438       1922      0.828      0.851      0.866      0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50         0G      0.506     0.6161     0.9599         88        640:  58%|█████▊    | 165/286 [15:04<11:02,  "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n-2.pt\")\n",
    "\n",
    "# Train the model\n",
    "train_results = model.train(\n",
    "    data=\"indoor_datasets/data.yaml\",  # path to dataset YAML\n",
    "    epochs=50,  # number of training epochs\n",
    "    imgsz=640,  # training image size\n",
    "    device=\"cpu\",  # device to run on, i.e. device=0 or device=0,1,2,3 or device=cpu\n",
    ")\n",
    "\n",
    "# Evaluate model performance on the validation set\n",
    "metrics = model.val()\n",
    "\n",
    "# Perform object detection on an image\n",
    "results = model(\"path/to/image.jpg\")\n",
    "results[0].show()\n",
    "\n",
    "# Export the model to ONNX format\n",
    "path = model.export(format=\"onnx\")  # return path to exported model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e47ca67-99b3-4b07-bf1c-01d5655fdf96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 person, 191.4ms\n",
      "Speed: 9.4ms preprocess, 191.4ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected person with confidence 0.7395603060722351\n",
      "\n",
      "0: 640x640 1 person, 150.1ms\n",
      "Speed: 4.3ms preprocess, 150.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected person with confidence 0.7578720450401306\n",
      "\n",
      "0: 640x640 3 persons, 1 cell phone, 160.9ms\n",
      "Speed: 4.9ms preprocess, 160.9ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected person with confidence 0.6161508560180664\n",
      "Detected cell phone with confidence 0.6091246604919434\n",
      "Detected person with confidence 0.5019234418869019\n",
      "Detected person with confidence 0.177542582154274\n",
      "\n",
      "0: 640x640 1 person, 131.3ms\n",
      "Speed: 4.0ms preprocess, 131.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected person with confidence 0.7331060767173767\n",
      "\n",
      "0: 640x640 4 persons, 1 cat, 1 cell phone, 155.3ms\n",
      "Speed: 4.7ms preprocess, 155.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected cell phone with confidence 0.3374802768230438\n",
      "Detected cat with confidence 0.24313801527023315\n",
      "Detected person with confidence 0.21319496631622314\n",
      "Detected person with confidence 0.19169627130031586\n",
      "Detected person with confidence 0.14318199455738068\n",
      "Detected person with confidence 0.1301085352897644\n",
      "\n",
      "0: 640x640 2 persons, 1 sports ball, 2 cell phones, 149.3ms\n",
      "Speed: 5.0ms preprocess, 149.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected cell phone with confidence 0.43664535880088806\n",
      "Detected cell phone with confidence 0.2102365642786026\n",
      "Detected sports ball with confidence 0.16282972693443298\n",
      "Detected person with confidence 0.14207831025123596\n",
      "Detected person with confidence 0.12147399038076401\n",
      "\n",
      "0: 640x640 1 person, 158.3ms\n",
      "Speed: 6.0ms preprocess, 158.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected person with confidence 0.839152991771698\n",
      "\n",
      "0: 640x640 2 persons, 1 cell phone, 147.6ms\n",
      "Speed: 4.7ms preprocess, 147.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected cell phone with confidence 0.7422239184379578\n",
      "Detected person with confidence 0.3233290910720825\n",
      "Detected person with confidence 0.28056368231773376\n",
      "\n",
      "0: 640x640 1 person, 158.8ms\n",
      "Speed: 4.0ms preprocess, 158.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected person with confidence 0.8445655107498169\n",
      "\n",
      "0: 640x640 1 person, 162.3ms\n",
      "Speed: 6.0ms preprocess, 162.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected person with confidence 0.8054435849189758\n",
      "\n",
      "0: 640x640 1 person, 127.2ms\n",
      "Speed: 3.7ms preprocess, 127.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected person with confidence 0.7227495908737183\n",
      "\n",
      "0: 640x640 1 person, 155.7ms\n",
      "Speed: 4.0ms preprocess, 155.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected person with confidence 0.7889344096183777\n",
      "\n",
      "0: 640x640 1 person, 153.5ms\n",
      "Speed: 4.5ms preprocess, 153.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Detected person with confidence 0.8006331324577332\n",
      "Escape hit, closing\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import supervision as sv\n",
    "from ultralytics import YOLO  # Assuming you're using YOLOv8 for this task\n",
    "import pyttsx3  # For text-to-speech (audio feedback)\n",
    "\n",
    "# Load your trained YOLOv8 model (ensure correct path)\n",
    "model = YOLO(\"best.pt\")\n",
    "\n",
    "# List of class names (replace with your own classes if using a custom-trained model)\n",
    "class_names = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \n",
    "    \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \n",
    "    \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \n",
    "    \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \n",
    "    \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \n",
    "    \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \n",
    "    \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \n",
    "    \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \n",
    "    \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \n",
    "    \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \n",
    "    \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \n",
    "    \"toothbrush\"\n",
    "]\n",
    "\n",
    "\n",
    "# Step 6: Initialize text-to-speech engine for audio feedback\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Step 2: Data Acquisition (Open webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Unable to read camera feed\")\n",
    "    exit()\n",
    "\n",
    "# Initialize annotators\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "while True:\n",
    "    # Step 2: Capture frame-by-frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Step 3: Preprocessing (resize and noise reduction)\n",
    "    frame = cv2.resize(frame, (640, 640))  # Resize the image to match the model input size\n",
    "    frame = cv2.GaussianBlur(frame, (5, 5), 0)  # Apply Gaussian blur to reduce noise\n",
    "\n",
    "    # Step 4: Object Detection\n",
    "    results = model.predict(frame, conf=0.1)  # Perform inference with confidence threshold of 0.1\n",
    "\n",
    "    # Step 5: Post Processing (filter results)\n",
    "    detections = sv.Detections.from_ultralytics(results[0])  # Convert YOLO results to Supervision format\n",
    "\n",
    "    # Annotate the image with bounding boxes and labels\n",
    "    annotated_image = box_annotator.annotate(scene=frame, detections=detections)\n",
    "    annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)\n",
    "    \n",
    "    # Display the annotated frame\n",
    "    cv2.imshow('Webcam', annotated_image)\n",
    "\n",
    "    # Step 6: Feedback Mechanism (Text-to-Speech for detected objects)\n",
    "    detected_class_ids = detections.class_id  # Get class IDs from detections\n",
    "    if len(detected_class_ids) > 0:\n",
    "        detected_objects = [class_names[class_id] for class_id in detected_class_ids]\n",
    "        detected_classes_str = \", \".join(detected_objects)\n",
    "        engine.say(f\"Detected: {detected_classes_str}\")\n",
    "        engine.runAndWait()\n",
    "    \n",
    "    # Step 7: Data Logging (Performance metrics)\n",
    "    for i, class_id in enumerate(detected_class_ids):\n",
    "        print(f\"Detected {class_names[class_id]} with confidence {detections.confidence[i]}\")\n",
    "\n",
    "    # Step 6: Adjust timing between each detection to avoid too frequent updates\n",
    "    time.sleep(0.5)  # Add a delay of 0.5 seconds between frames for smoother processing\n",
    "\n",
    "    # Break the loop if ESC is pressed\n",
    "    k = cv2.waitKey(2)\n",
    "    if k % 256 == 27:  # ESC key\n",
    "        print(\"Escape hit, closing\")\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d550b7-1c6b-4d72-a0c4-d2f36244fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import supervision as sv\n",
    "from ultralytics import YOLO  # Assuming you're using YOLOv8 for this task\n",
    "import pyttsx3  # For text-to-speech (audio feedback)\n",
    "\n",
    "# Load your trained YOLOv8 model (ensure correct path)\n",
    "model = YOLO(\"yolov8nfinal.pt\")\n",
    "\n",
    "# List of class names (replace with your own classes if using a custom-trained model)\n",
    "class_names = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \n",
    "    \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \n",
    "    \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \n",
    "    \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \n",
    "    \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \n",
    "    \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \n",
    "    \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \n",
    "    \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \n",
    "    \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \n",
    "    \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \n",
    "    \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \n",
    "    \"toothbrush\"\n",
    "]\n",
    "\n",
    "# Initialize text-to-speech engine for audio feedback\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Open webcam for data acquisition\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Unable to read camera feed\")\n",
    "    exit()\n",
    "\n",
    "# Initialize annotators\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Preprocessing\n",
    "    frame = cv2.resize(frame, (640, 640))  # Resize the image to match the model input size\n",
    "    frame = cv2.GaussianBlur(frame, (5, 5), 0)  # Apply Gaussian blur to reduce noise\n",
    "\n",
    "    # Object Detection\n",
    "    results = model.predict(frame, conf=0.1)  # Perform inference with confidence threshold of 0.1\n",
    "    detections = sv.Detections.from_ultralytics(results[0])  # Convert YOLO results to Supervision format\n",
    "\n",
    "    # Annotate image with bounding boxes and labels\n",
    "    for i, detection in enumerate(detections):\n",
    "        class_id = detection.class_id\n",
    "        confidence = detection.confidence\n",
    "        class_name = class_names[class_id]\n",
    "        label = f\"{class_name}: {confidence:.2f}\"  # Include confidence score in label\n",
    "\n",
    "        # Annotate each detection with its label\n",
    "        box_annotator.annotate(scene=frame, detection=detection, label=label)\n",
    "\n",
    "    # Display the annotated frame\n",
    "    cv2.imshow('Webcam', frame)\n",
    "\n",
    "    # Text-to-Speech for detected objects\n",
    "    detected_class_ids = detections.class_id  # Get class IDs from detections\n",
    "    if len(detected_class_ids) > 0:\n",
    "        detected_objects = [f\"{class_names[class_id]} ({detections.confidence[i]:.2f})\" for i, class_id in enumerate(detected_class_ids)]\n",
    "        detected_classes_str = \", \".join(detected_objects)\n",
    "        engine.say(f\"Detected: {detected_classes_str}\")\n",
    "        engine.runAndWait()\n",
    "    \n",
    "    # Data Logging (Performance metrics)\n",
    "    for i, class_id in enumerate(detected_class_ids):\n",
    "        print(f\"Detected {class_names[class_id]} with confidence {detections.confidence[i]:.2f}\")\n",
    "\n",
    "    # Adjust timing between each detection to avoid too frequent updates\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    # Break the loop if ESC is pressed\n",
    "    k = cv2.waitKey(2)\n",
    "    if k % 256 == 27:  # ESC key\n",
    "        print(\"Escape hit, closing\")\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
